{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _base_path\n",
    "import pickle\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n",
    "from resources.data_io import load_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA      = 'incidents'\n",
    "MODEL     = 'tfidf-lr'\n",
    "LABEL     = 'hazard-category'\n",
    "TASK      = LABEL.split(\"-\")[0]\n",
    "CV_SPLITS = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Class-Mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = load_mappings(f\"../data/{DATA}/splits/\", LABEL)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/{DATA}/support_zones.json', 'r') as file:\n",
    "    high_support, low_support = json.load(file)[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv(f'../data/{DATA}/{DATA}_final.csv')[LABEL].value_counts()\n",
    "\n",
    "class_map = list(zip(\n",
    "    class_map,\n",
    "    range(len(class_map)),\n",
    "    [counts[c] if c in counts else 0 for c in class_map]\n",
    "))\n",
    "class_map.sort(key=lambda row:row[1])\n",
    "class_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct   = []\n",
    "incorrect = []\n",
    "\n",
    "for i in CV_SPLITS:\n",
    "    try:\n",
    "        with open(f'../data/{DATA}/splits/split_{TASK}_{i:d}.pickle', 'rb') as f:\n",
    "            texts = pickle.load(f)['test'][[LABEL, TASK+'-title', 'title']]\n",
    "\n",
    "        with open(f'{MODEL}/{MODEL}-{LABEL}-{i:d}.pickle', 'rb') as f:\n",
    "            predictions = pickle.load(f)\n",
    "\n",
    "        assert all(texts[LABEL] == predictions['labels'])\n",
    "        texts.rename(columns={LABEL:'y_true', TASK+'-title':'spans'}, inplace=True)\n",
    "        texts['y_pred'] = predictions['predictions']\n",
    "\n",
    "        correct.append(texts[predictions['labels'] == predictions['predictions']])\n",
    "        incorrect.append(texts[predictions['labels'] != predictions['predictions']])\n",
    "\n",
    "    except FileNotFoundError: continue\n",
    "\n",
    "correct = pd.concat(correct)\n",
    "incorrect = pd.concat(incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctly classified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(correct.index)\n",
    "txt = nlp(correct.title[i])\n",
    "txt.spans[\"sc\"] = []\n",
    "\n",
    "sup = correct.spans[i]\n",
    "ent = LABEL[:3].upper()\n",
    "print('\\n', ent, '\\n y_true ->', class_map[correct.y_true[i]], '\\n y_pred ->', class_map[correct.y_pred[i]])\n",
    "\n",
    "spans = [txt.char_span(l.start,l.stop,ent) for l in sup]\n",
    "for span in [correct['title'][i][l] for l in sup]:\n",
    "    print('[...]', span, '[...]')\n",
    "\n",
    "#txt.set_ents(txt.ents + tuple([span for span in spans if span is not None]))\n",
    "txt.spans[\"sc\"] = tuple(txt.spans[\"sc\"]) + tuple([span for span in spans if span is not None])\n",
    "\n",
    "#displacy.render(txt, style='ent')\n",
    "displacy.render(txt, style='span')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrectly classified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(incorrect.index)\n",
    "txt = nlp(incorrect.title[i])\n",
    "txt.spans[\"sc\"] = []\n",
    "\n",
    "sup = incorrect.spans[i]\n",
    "ent = LABEL[:3].upper()\n",
    "print('\\n', ent, '\\n y_true ->', class_map[incorrect.y_true[i]], '\\n y_pred ->', class_map[incorrect.y_pred[i]])\n",
    "\n",
    "spans = [txt.char_span(l.start,l.stop,ent) for l in sup]\n",
    "for span in [incorrect['title'][i][l] for l in sup]:\n",
    "    print('[...]', span, '[...]')\n",
    "\n",
    "#txt.set_ents(txt.ents + tuple([span for span in spans if span is not None]))\n",
    "txt.spans[\"sc\"] = tuple(txt.spans[\"sc\"]) + tuple([span for span in spans if span is not None])\n",
    "\n",
    "#displacy.render(txt, style='ent')\n",
    "displacy.render(txt, style='span')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('semeval': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b666b6ffdea20fd94a09543e671feb1173b4f04c4549159b4e01bb8315246c89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
