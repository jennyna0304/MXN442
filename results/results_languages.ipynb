{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting base bath to \"c:\\Users\\Korbi\\Desktop\\CICLe\"\n"
     ]
    }
   ],
   "source": [
    "import _base_path\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from resources.data_io import load_mappings\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA      = 'incidents'\n",
    "MODELS    = ['roberta-base', 'xlm-roberta-base']\n",
    "LANGUAGES = ['en', 'de']\n",
    "LABEL     = 'hazard-category'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Class-Mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['allergens', 'biological', 'chemical',\n",
       "       'food additives and flavourings', 'food contact materials',\n",
       "       'foreign bodies', 'fraud', 'migration', 'organoleptic aspects',\n",
       "       'other hazard', 'packaging defect'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_map = load_mappings(f\"../data/{DATA}/splits/\", LABEL)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/{DATA}/support_zones.json', 'r') as file:\n",
    "    high_support, low_support = json.load(file)[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biological']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foreign bodies',\n",
       " 'chemical',\n",
       " 'fraud',\n",
       " 'other hazard',\n",
       " 'packaging defect',\n",
       " 'organoleptic aspects',\n",
       " 'food additives and flavourings',\n",
       " 'migration',\n",
       " 'food contact materials']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biological', 1, 2558),\n",
       " ('allergens', 0, 2527),\n",
       " ('foreign bodies', 5, 943),\n",
       " ('chemical', 2, 578),\n",
       " ('fraud', 6, 527),\n",
       " ('other hazard', 9, 187),\n",
       " ('packaging defect', 10, 100),\n",
       " ('organoleptic aspects', 8, 81),\n",
       " ('food additives and flavourings', 3, 32),\n",
       " ('migration', 7, 14),\n",
       " ('food contact materials', 4, 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.read_csv(f'../data/{DATA}/{DATA}_final.csv')[LABEL].value_counts()\n",
    "\n",
    "class_map = list(zip(\n",
    "    class_map,\n",
    "    range(len(class_map)),\n",
    "    [counts[c] if c in counts else 0 for c in class_map]\n",
    "))\n",
    "class_map.sort(key=lambda row:row[2], reverse=True)\n",
    "class_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for m in MODELS:\n",
    "    try:\n",
    "        for i in range(5):\n",
    "            with open(f'../results/{m}/{m}-{LABEL}-{i:d}.pickle', 'rb') as f:\n",
    "                r = pickle.load(f)\n",
    "\n",
    "            with open(f'../data/{DATA}/splits/split_{LABEL.split(\"-\")[0]}_{i:d}.pickle', 'rb') as f:\n",
    "                l = pickle.load(f)['test']['language'].values\n",
    "\n",
    "            for ln in np.unique(l):\n",
    "                if m not in results:     results[m] = {}\n",
    "                if ln not in results[m]: results[m][ln] = []\n",
    "\n",
    "                results[m][ln].append({\n",
    "                    'labels':      r['labels'][l==ln],\n",
    "                    'predictions': r['predictions'][l==ln]\n",
    "                })\n",
    "\n",
    "    except FileNotFoundError: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(classes=[c for c, _, _ in class_map]):\n",
    "    classes = [i for c, i, _ in class_map if c in classes]\n",
    "    metrics = {}\n",
    "\n",
    "    for model in results:\n",
    "        metrics[model] = {language: np.empty(5, dtype=float) for language in LANGUAGES}\n",
    "\n",
    "        for language in LANGUAGES:\n",
    "\n",
    "            for i, r in enumerate(results[model][language]):\n",
    "                mask = np.vectorize(lambda c: c in classes)(r['labels'])\n",
    "                y_true = r['labels'][mask]\n",
    "                y_pred = r['predictions'][mask]\n",
    "\n",
    "                metrics[model][language][i] = f1_score(y_true, y_pred, average='macro', zero_division=0.0)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = calculate_metrics()\n",
    "metrics_high_support = calculate_metrics(high_support)\n",
    "metrics_low_support = calculate_metrics(low_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2latex(metrics_dict): \n",
    "    metrics = np.array([[metrics_dict[model][language] for language in LANGUAGES] for model in metrics_dict], dtype=float)\n",
    "    avg     = metrics.mean(axis=-1)\n",
    "    err     = np.abs(metrics - avg.reshape(avg.shape + (1,))).mean(axis=-1)\n",
    "\n",
    "    best    = np.round(avg, 2) == np.round(np.max(avg, axis=0), 2)\n",
    "\n",
    "    return np.vectorize(\n",
    "        lambda a, e, b: f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$ \\\\tiny $\\\\bf\\\\pm {e:.2f}$' if b else  f'\\\\footnotesize ${a:.2f}$ \\\\tiny $\\\\pm {e:.2f}$'\n",
    "    )(avg, err, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBERTA-BASE &\n",
      "\\cellcolor{blue!15}\\footnotesize $\\bf 0.64$ \\tiny $\\bf\\pm 0.06$ & \\cellcolor{blue!15}\\footnotesize $\\bf 0.22$ \\tiny $\\bf\\pm 0.03$ &\n",
      "\\cellcolor{blue!15}\\footnotesize $\\bf 0.14$ \\tiny $\\bf\\pm 0.03$ & \\footnotesize $0.14$ \\tiny $\\pm 0.03$ &\n",
      "\\cellcolor{blue!15}\\footnotesize $\\bf 0.48$ \\tiny $\\bf\\pm 0.05$ & \\cellcolor{blue!15}\\footnotesize $\\bf 0.16$ \\tiny $\\bf\\pm 0.03$ \\\\\n",
      "\n",
      "XLM-ROBERTA-BASE &\n",
      "\\cellcolor{blue!15}\\footnotesize $\\bf 0.64$ \\tiny $\\bf\\pm 0.05$ & \\footnotesize $0.21$ \\tiny $\\pm 0.02$ &\n",
      "\\cellcolor{blue!15}\\footnotesize $\\bf 0.14$ \\tiny $\\bf\\pm 0.01$ & \\cellcolor{blue!15}\\footnotesize $\\bf 0.17$ \\tiny $\\bf\\pm 0.02$ &\n",
      "\\cellcolor{blue!15}\\footnotesize $\\bf 0.48$ \\tiny $\\bf\\pm 0.05$ & \\footnotesize $0.15$ \\tiny $\\pm 0.02$ \\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ltx_all = metric2latex(metrics_all)\n",
    "ltx_hs  = metric2latex(metrics_high_support)\n",
    "ltx_ls  = metric2latex(metrics_low_support)\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    row =  f'{model.upper()} &\\n'\n",
    "\n",
    "    if model in metrics_all:            row += ' & '.join(ltx_all[i])\n",
    "    else:                               row += ' &'*(len(LANGUAGES)-1)\n",
    "    row += ' &\\n'\n",
    "\n",
    "    if model in metrics_high_support:   row += ' & '.join(ltx_hs[i])\n",
    "    else:                               row += ' &'*(len(LANGUAGES)-1)\n",
    "    row += ' &\\n'\n",
    "\n",
    "    if model in metrics_low_support:    row += ' & '.join(ltx_ls[i])\n",
    "    else:                               row += ' &'*(len(LANGUAGES)-1)\n",
    "    row += ' \\\\\\\\\\n'\n",
    "\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('semeval': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b666b6ffdea20fd94a09543e671feb1173b4f04c4549159b4e01bb8315246c89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
