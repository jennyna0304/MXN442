{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _base_path\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from resources.data_io import load_mappings\n",
    "from resources.metrics import ConfusionMatrix\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA                = 'incidents'\n",
    "LABEL               = 'hazard-category'\n",
    "MODEL               = 'gpt-3.5-turbo-instruct'\n",
    "SHOTS               = 2\n",
    "METRICS             = {\n",
    "    'micro-f1':     lambda y_true, y_pred: f1_score(y_true, y_pred, average='micro', zero_division=0.0),\n",
    "    'macro-f1':     lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "    'recall':       lambda y_true, y_pred: recall_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "    'precision':    lambda y_true, y_pred: precision_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "#    'accuracy':     lambda y_true, y_pred: accuracy_score(y_true, y_pred)\n",
    "}\n",
    "CV_SPLITS           = [0]#, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Class-Mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = load_mappings(f'../data/{DATA}/splits/', LABEL)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv(f'../data/{DATA}/{DATA}_final.csv')[LABEL].value_counts()\n",
    "\n",
    "class_map = list(zip(\n",
    "    class_map,\n",
    "    range(len(class_map)),\n",
    "    [counts[c] if c in counts else 0 for c in class_map]\n",
    "))\n",
    "class_map.sort(key=lambda row:row[2], reverse=True)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_all = [c for c, _, n in class_map if n > 0]\n",
    "classes_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/{DATA}/support_zones.json', 'r') as file:\n",
    "    classes_high_support, classes_low_support = json.load(file)[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_high_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_low_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in CV_SPLITS:\n",
    "    with open(f\"../data/{DATA}/splits/split_{LABEL.split('-')[0]}_{split:d}.pickle\", \"rb\") as f:\n",
    "        # load data for split:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "        # get unique classes in train and test sets:\n",
    "        c_train = [c for c, i, _ in class_map if sum(data['train'][LABEL].values == i) >= 4]\n",
    "        c_test  = [c for c, i, _ in class_map if sum(data['test'][LABEL].values == i) >= 1]\n",
    "\n",
    "    # only use classes that are present in the train AND test set:\n",
    "    classes_all          = [c for c in classes_all if c in c_train and c in c_test]\n",
    "    classes_high_support = [c for c in classes_high_support if c in c_train and c in c_test]\n",
    "    classes_low_support  = [c for c in classes_low_support if c in c_train and c in c_test]\n",
    "\n",
    "len(classes_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(f'../results/{MODEL}/{MODEL}_{LABEL}_{SHOTS:d}-shot.csv').fillna('')\n",
    "results = results[['cv_split', 'label'] + [col for col in results.columns if col.startswith('output_')]]\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = pd.read_csv(f'../prompts/prompts_{LABEL}_{SHOTS:d}-shot.csv').fillna('')\n",
    "limits = limits[['cv_split', 'label'] + [col for col in limits.columns if col.startswith('output_')]]\n",
    "\n",
    "limits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, classes, filter_empty=False):\n",
    "    metrics = {}\n",
    "\n",
    "    for col in df.drop(columns=['cv_split', 'label']).columns:\n",
    "        if   col.startswith('output_raw_'): continue\n",
    "        elif col.startswith('output_min_'): group = 'min'\n",
    "        elif col.startswith('output_max_'): group = 'max'\n",
    "        else:                               group = 'model'\n",
    "\n",
    "        if group not in metrics: metrics[group] = {}\n",
    "\n",
    "        metrics[group][col] = {metric: np.empty(len(CV_SPLITS), dtype=float) for metric in METRICS}\n",
    "\n",
    "        for split in CV_SPLITS:\n",
    "            r = df[df['cv_split'] == split][['label', col]].values\n",
    "\n",
    "            if (r[:,1] == '').all():\n",
    "                print(f'Skipping split {split:d} of column \"{col}\"')\n",
    "                continue\n",
    "\n",
    "            if filter_empty:\n",
    "                r = r[r[:,1] != '']\n",
    "\n",
    "            mask = np.vectorize(lambda c: c in classes)(r[:,0])\n",
    "            y_true = np.stack([r[mask, 0] == c for c in classes], dtype=int, axis=1)\n",
    "            y_pred = np.stack([r[mask, 1] == c for c in classes], dtype=int, axis=1)\n",
    "\n",
    "            for metric in metrics[group][col]:\n",
    "                metrics[group][col][metric][split] = METRICS[metric](y_true, y_pred)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = calculate_metrics(limits, classes_all)\n",
    "metrics_all.update(calculate_metrics(results, classes_all))\n",
    "\n",
    "metrics_high_support = calculate_metrics(limits, classes_high_support)\n",
    "metrics_high_support.update(calculate_metrics(results, classes_high_support))\n",
    "\n",
    "metrics_low_support = calculate_metrics(limits, classes_low_support)\n",
    "metrics_low_support.update(calculate_metrics(results, classes_low_support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2latex(metrics_dict, report_max=False): \n",
    "    metrics = np.array([[metrics_dict[model][metric] for metric in metrics_dict[model]] for model in metrics_dict], dtype=float)\n",
    "    \n",
    "    avg     = metrics.mean(axis=-1)\n",
    "    best    = np.round(avg, 2) == np.round(np.max(avg, axis=0), 2)\n",
    "    if metrics.shape[-1] == 1: return np.vectorize(\n",
    "        lambda a, b:    f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$'\n",
    "                        if b else  f'\\\\footnotesize ${a:.2f}$'\n",
    "    )(avg, best)\n",
    "\n",
    "    if report_max:\n",
    "        return np.vectorize(\n",
    "            lambda a, m, b: f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$ & \\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {m:.2f}$'\n",
    "                            if b else f'\\\\footnotesize ${a:.2f}$ & \\\\footnotesize ${m:.2f}$'\n",
    "        )(avg, metrics.max(axis=-1), best)\n",
    "\n",
    "    else:\n",
    "        err     = np.abs(metrics - avg.reshape(avg.shape + (1,))).mean(axis=-1)\n",
    "        return np.vectorize(\n",
    "            lambda a, e, b: f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$ \\\\tiny $\\\\bf\\\\pm {e:.2f}$'\n",
    "                            if b else f'\\\\footnotesize ${a:.2f}$ \\\\tiny $\\\\pm {e:.2f}$'\n",
    "        )(avg, err, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in ['min', 'max', 'model']:\n",
    "\n",
    "    ltx_all = metric2latex(metrics_all[group])\n",
    "    ltx_hs  = metric2latex(metrics_high_support[group])\n",
    "    ltx_ls  = metric2latex(metrics_low_support[group])\n",
    "\n",
    "    for i, col in enumerate(metrics_all[group]):\n",
    "        row =  f'{col.upper()} &\\n'\n",
    "\n",
    "        if col in metrics_all[group]:           row += ' & '.join(ltx_all[i])\n",
    "        else:                                   row += ' &'*(len(METRICS)-1)\n",
    "        row += ' &\\n'\n",
    "\n",
    "        if col in metrics_high_support[group]:  row += ' & '.join(ltx_hs[i])\n",
    "        else:                                   row += ' &'*(len(METRICS)-1)\n",
    "        row += ' &\\n'\n",
    "\n",
    "        if col in metrics_low_support[group]:   row += ' & '.join(ltx_ls[i])\n",
    "        else:                                   row += ' &'*(len(METRICS)-1)\n",
    "        row += ' \\\\\\\\\\n'\n",
    "\n",
    "        print(row)\n",
    "\n",
    "    print('\\\\hline\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_types = class_map.copy()\n",
    "label_types.sort(key=lambda item:item[1])\n",
    "label_types = [item[0] for item in label_types]\n",
    "\n",
    "label_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in limits.columns:\n",
    "    if col.startswith('output_raw_'):\n",
    "        mask = [limits[limits.cv_split == i][col].apply(lambda p: p in label_types) for i in CV_SPLITS]\n",
    "        print(f'{col[11:].upper()}: ${sum(~mask[0]) / len(mask[0]) * 100:.0f}\\%$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in results.columns:\n",
    "    mask = [results[results.cv_split == i][col].apply(lambda p: p in label_types) for i in CV_SPLITS]\n",
    "#    mask = [results[results.cv_split == i][col].apply(lambda p: True) for i in CV_SPLITS]\n",
    "\n",
    "    f1 = [f1_score(\n",
    "        results[results.cv_split == i].label.values[mask[i]],\n",
    "        results[results.cv_split == i][col].values[mask[i]],\n",
    "        average='macro',\n",
    "        zero_division=0\n",
    "    ) for i in CV_SPLITS]\n",
    "\n",
    "    acc = [accuracy_score(\n",
    "        results[results.cv_split == i].label.values[mask[i]],\n",
    "        results[results.cv_split == i][col].values[mask[i]]\n",
    "    ) for i in CV_SPLITS]\n",
    "\n",
    "    fail = [np.mean(~mask[i]) for i in CV_SPLITS]\n",
    "\n",
    "    empty = [np.mean(\n",
    "        results[results.cv_split == i][col].apply(lambda p: p == '')\n",
    "    ) for i in CV_SPLITS]\n",
    "\n",
    "    print(f'{col.upper()}:')\n",
    "    print(f'  F1:       {np.mean(f1):.2f} \\u00b1 {np.std(f1):.2f}')\n",
    "    print(f'  Accuracy: {np.mean(acc):.2f} \\u00b1 {np.std(acc):.2f}')\n",
    "    print(f'  Failed:   {np.mean(fail):.2f} \\u00b1 {np.std(fail):.2f}')\n",
    "    print(f'  Empty:    {np.mean(empty):.2f} \\u00b1 {np.std(empty):.2f}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = {}\n",
    "for col in results.columns:\n",
    "    fails[col] = []\n",
    "    for split in CV_SPLITS:\n",
    "        try:\n",
    "            task = LABEL.split('-')[0]\n",
    "\n",
    "            with open(f'../data/{DATA}/splits/split_{task}_{split:d}.pickle', 'rb') as f:\n",
    "                texts = pickle.load(f)['test'][[LABEL, task + '-title', 'title']]\n",
    "\n",
    "            labels = results[results.cv_split == split]['label'].values\n",
    "            preds  = results[results.cv_split == split][col].values\n",
    "\n",
    "            assert all([label_types[i] for i in texts[LABEL]] == labels)\n",
    "\n",
    "            mask = np.vectorize(lambda p: p in label_types)(preds)\n",
    "            \n",
    "            fails[col].append(list(zip(\n",
    "                texts['title'].values[~mask],\n",
    "                labels[~mask],\n",
    "                preds[~mask]\n",
    "            )))\n",
    "\n",
    "        except FileNotFoundError: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in fails['output_sim-20'][0] if item[1] in classes_low_support]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results[results['cv_split'] == 0][['label', 'output_conformal_5%']].values\n",
    "r = r[r[:,1] != '']\n",
    "\n",
    "for i in [0,1]:\n",
    "    hs_mask = np.array([s in classes_high_support for s in r[:,i]])\n",
    "    ls_mask = np.array([s in classes_low_support for s in r[:,i]])\n",
    "    ms_mask = ~(hs_mask | ls_mask)\n",
    "\n",
    "    r[hs_mask, i] = 0\n",
    "    r[ms_mask, i] = 1\n",
    "    r[ls_mask, i] = 2\n",
    "\n",
    "cm = ConfusionMatrix(r[:,0], r[:,1], classes=[\"High\", \"Medium\", \"Low\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(3, 3))\n",
    "cm.plot(axs)\n",
    "fig.savefig(f'../pictures/plots/cm_conformal_{LABEL}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
