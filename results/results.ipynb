{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _base_path\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from resources.data_io import load_mappings\n",
    "from resources.metrics import ConfusionMatrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA                = 'incidents'\n",
    "MODELS              = [\n",
    "    'bow-rnd',\n",
    "    'bow-sup',\n",
    "    'bow-knn',\n",
    "    'bow-lr',\n",
    "    'bow-svm',\n",
    "    'tfidf-knn',\n",
    "    'tfidf-lr',\n",
    "    'tfidf-svm',\n",
    "    'roberta-base',\n",
    "    'xlm-roberta-base'\n",
    "]\n",
    "METRICS             = {\n",
    "    'micro-f1':     lambda y_true, y_pred: f1_score(y_true, y_pred, average='micro', zero_division=0.0),\n",
    "    'macro-f1':     lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "    'precision':    lambda y_true, y_pred: precision_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "    'recall':       lambda y_true, y_pred: recall_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "#    'accuracy':     lambda y_true, y_pred: accuracy_score(y_true, y_pred)\n",
    "}\n",
    "LABEL               = 'hazard-category'\n",
    "CV_SPLITS           = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Class-Mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = load_mappings(f\"../data/{DATA}/splits/\", LABEL)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv(f'../data/{DATA}/{DATA}_final.csv')[LABEL].value_counts()\n",
    "\n",
    "class_map = list(zip(\n",
    "    class_map,\n",
    "    range(len(class_map)),\n",
    "    [counts[c] if c in counts else 0 for c in class_map]\n",
    "))\n",
    "class_map.sort(key=lambda row:row[2], reverse=True)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_all = [c for c, _, n in class_map if n > 0]\n",
    "classes_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/{DATA}/support_zones.json', 'r') as file:\n",
    "    classes_high_support, classes_low_support = json.load(file)[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_high_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_low_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in CV_SPLITS:\n",
    "    with open(f\"../data/{DATA}/splits/split_{LABEL.split('-')[0]}_{split:d}.pickle\", \"rb\") as f:\n",
    "        # load data for split:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "        # get unique classes in train and test sets:\n",
    "        c_train = [c for c, i, _ in class_map if sum(data['train'][LABEL].values == i) >= 4]\n",
    "        c_test  = [c for c, i, _ in class_map if sum(data['test'][LABEL].values == i) >= 1]\n",
    "\n",
    "    # only use classes that are present in the train AND test set:\n",
    "    classes_all          = [c for c in classes_all if c in c_train and c in c_test]\n",
    "    classes_high_support = [c for c in classes_high_support if c in c_train and c in c_test]\n",
    "    classes_low_support  = [c for c in classes_low_support if c in c_train and c in c_test]\n",
    "\n",
    "len(classes_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for m in MODELS:\n",
    "    r = []\n",
    "    try:\n",
    "        for split in CV_SPLITS:\n",
    "            with open(f'{m}/{m}-{LABEL}-{split:d}.pickle', 'rb') as f:\n",
    "                r.append(pickle.load(f))\n",
    "    except FileNotFoundError: continue\n",
    "    results[m] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(classes):\n",
    "    classes = [i for c, i, _ in class_map if c in classes]\n",
    "    metrics = {}\n",
    "\n",
    "    for model in results:\n",
    "        metrics[model] = {metric: np.empty(len(CV_SPLITS), dtype=float) for metric in METRICS}\n",
    "\n",
    "        for split, r in enumerate(results[model]):\n",
    "            mask = np.vectorize(lambda c: c in classes)(r['labels'])\n",
    "            y_true = np.stack([r['labels'][mask] == c for c in classes], dtype=int, axis=1)\n",
    "            y_pred = np.stack([r['predictions'][mask] == c for c in classes], dtype=int, axis=1)\n",
    "\n",
    "            for metric in METRICS:\n",
    "                metrics[model][metric][split] = METRICS[metric](y_true, y_pred)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = calculate_metrics(classes_all)\n",
    "metrics_high_support = calculate_metrics(classes_high_support)\n",
    "metrics_low_support = calculate_metrics(classes_low_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2latex(metrics_dict, report_max=True): \n",
    "    metrics = np.array([[metrics_dict[model][metric] for metric in metrics_dict[model]] for model in metrics_dict], dtype=float)\n",
    "    \n",
    "    avg     = metrics.mean(axis=-1)\n",
    "    best    = np.round(avg, 2) == np.round(np.max(avg, axis=0), 2)\n",
    "    if metrics.shape[-1] == 1: return np.vectorize(\n",
    "        lambda a, b:    f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$'\n",
    "                        if b else  f'\\\\footnotesize ${a:.2f}$'\n",
    "    )(avg, best)\n",
    "\n",
    "    if report_max:\n",
    "        return np.vectorize(\n",
    "            lambda a, m, b: f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$ & \\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {m:.2f}$'\n",
    "                            if b else f'\\\\footnotesize ${a:.2f}$ & \\\\footnotesize ${m:.2f}$'\n",
    "        )(avg, metrics.max(axis=-1), best)\n",
    "\n",
    "    else:\n",
    "        err     = np.abs(metrics - avg.reshape(avg.shape + (1,))).mean(axis=-1)\n",
    "        return np.vectorize(\n",
    "            lambda a, e, b: f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$ \\\\tiny $\\\\bf\\\\pm {e:.2f}$'\n",
    "                            if b else f'\\\\footnotesize ${a:.2f}$ \\\\tiny $\\\\pm {e:.2f}$'\n",
    "        )(avg, err, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltx_all = metric2latex(metrics_all)\n",
    "ltx_hs  = metric2latex(metrics_high_support)\n",
    "ltx_ls  = metric2latex(metrics_low_support)\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    row =  f'{model.upper()} &\\n'\n",
    "\n",
    "    if model in metrics_all:            row += ' & '.join(ltx_all[i])\n",
    "    else:                               row += ' &'*(len(METRICS)-1)\n",
    "    row += ' &\\n'\n",
    "\n",
    "    if model in metrics_high_support:   row += ' & '.join(ltx_hs[i])\n",
    "    else:                               row += ' &'*(len(METRICS)-1)\n",
    "    row += ' &\\n'\n",
    "\n",
    "    if model in metrics_low_support:    row += ' & '.join(ltx_ls[i])\n",
    "    else:                               row += ' &'*(len(METRICS)-1)\n",
    "    row += ' \\\\\\\\\\n'\n",
    "\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'tfidf-lr'\n",
    "\n",
    "hs_mask = np.zeros(len(class_map), dtype=bool)\n",
    "ls_mask = np.zeros(len(class_map), dtype=bool)\n",
    "ms_mask = np.zeros(len(class_map), dtype=bool)\n",
    "\n",
    "for c, i, _ in class_map:\n",
    "    hs_mask[i] = c in classes_high_support\n",
    "    ls_mask[i] = c in classes_low_support\n",
    "    ms_mask[i] = not(hs_mask[i] or ls_mask[i])\n",
    "        \n",
    "y_pred = np.array([\n",
    "    np.argmax([hs_mask[e], ms_mask[e], ls_mask[e]])\n",
    "    for e in results[MODEL][0]['predictions']\n",
    "])\n",
    "y_true = np.array([\n",
    "    np.argmax([hs_mask[e], ms_mask[e], ls_mask[e]])\n",
    "    for e in results[MODEL][0]['labels']\n",
    "])\n",
    "\n",
    "cm = ConfusionMatrix(y_true, y_pred, classes=[\"High\", \"Medium\", \"Low\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(3, 3))\n",
    "cm.plot(axs)\n",
    "fig.savefig(f'../pictures/plots/cm_{MODEL}_{LABEL}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('semeval': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b666b6ffdea20fd94a09543e671feb1173b4f04c4549159b4e01bb8315246c89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
