{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _base_path\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from resources.data_io import load_mappings\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA              = 'incidents'\n",
    "MODELS            = ['roberta-base', 'xlm-roberta-base']\n",
    "LABEL             = 'hazard-category'\n",
    "METRICS           = {\n",
    "    'micro-f1':     lambda y_true, y_pred: f1_score(y_true, y_pred, average='micro', zero_division=0.0),\n",
    "    'macro-f1':     lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "    'recall':       lambda y_true, y_pred: recall_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "    'precision':    lambda y_true, y_pred: precision_score(y_true, y_pred, average='macro', zero_division=0.0),\n",
    "#    'accuracy':     lambda y_true, y_pred: accuracy_score(y_true, y_pred)\n",
    "}\n",
    "CV_SPLITS         = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Class-Mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = load_mappings(f\"../data/{DATA}/splits/\", LABEL)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv(f'../data/{DATA}/{DATA}_final.csv')[LABEL].value_counts()\n",
    "\n",
    "class_map = list(zip(\n",
    "    class_map,\n",
    "    range(len(class_map)),\n",
    "    [counts[c] if c in counts else 0 for c in class_map]\n",
    "))\n",
    "class_map.sort(key=lambda row:row[2], reverse=True)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_all = [c for c, _, n in class_map if n > 0]\n",
    "classes_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in CV_SPLITS:\n",
    "    with open(f\"../data/{DATA}/splits/split_{LABEL.split('-')[0]}_{split:d}.pickle\", \"rb\") as f:\n",
    "        # load data for split:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "        # get unique classes in train and test sets:\n",
    "        c_train = [c for c, i, _ in class_map if sum(data['train'][LABEL].values == i) >= 4]\n",
    "        c_test  = [c for c, i, _ in class_map if sum(data['test'][LABEL].values == i) >= 1]\n",
    "\n",
    "    # only use classes that are present in the train AND test set:\n",
    "    classes_all          = [c for c in classes_all if c in c_train and c in c_test]\n",
    "\n",
    "len(classes_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for m in MODELS:\n",
    "    try:\n",
    "        for split in CV_SPLITS:\n",
    "            with open(f'../results/{m}/{m}-{LABEL}-{split:d}.pickle', 'rb') as f:\n",
    "                r = pickle.load(f)\n",
    "\n",
    "            with open(f'../data/{DATA}/splits/split_{LABEL.split(\"-\")[0]}_{split:d}.pickle', 'rb') as f:\n",
    "                l = pickle.load(f)['test']['language'].values\n",
    "\n",
    "            for ln in np.unique(l):\n",
    "                if m not in results:     results[m] = {}\n",
    "                if ln not in results[m]: results[m][ln] = []\n",
    "\n",
    "                results[m][ln].append({\n",
    "                    'labels':      r['labels'][l==ln],\n",
    "                    'predictions': r['predictions'][l==ln]\n",
    "                })\n",
    "\n",
    "    except FileNotFoundError: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(classes, language):\n",
    "    classes = [i for c, i, _ in class_map if c in classes]\n",
    "    metrics = {}\n",
    "\n",
    "    for model in results:\n",
    "        metrics[model] = {metric: np.empty(len(CV_SPLITS), dtype=float) for metric in METRICS}\n",
    "\n",
    "        for split, r in enumerate(results[model][language]):\n",
    "            mask = np.vectorize(lambda c: c in classes)(r['labels'])\n",
    "            y_true = np.stack([r['labels'][mask] == c for c in classes], dtype=int, axis=1)\n",
    "            y_pred = np.stack([r['predictions'][mask] == c for c in classes], dtype=int, axis=1)\n",
    "\n",
    "            for metric in METRICS:\n",
    "                metrics[model][metric][split] = METRICS[metric](y_true, y_pred)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_en = calculate_metrics(classes_all, 'en')\n",
    "metrics_de = calculate_metrics(classes_all, 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2latex(metrics_dict, report_max=False): \n",
    "    metrics = np.array([[metrics_dict[model][metric] for metric in metrics_dict[model]] for model in metrics_dict], dtype=float)\n",
    "    \n",
    "    avg     = metrics.mean(axis=-1)\n",
    "    best    = np.round(avg, 2) == np.round(np.max(avg, axis=0), 2)\n",
    "    if metrics.shape[-1] == 1: return np.vectorize(\n",
    "        lambda a, b:    f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$'\n",
    "                        if b else  f'\\\\footnotesize ${a:.2f}$'\n",
    "    )(avg, best)\n",
    "\n",
    "    if report_max:\n",
    "        return np.vectorize(\n",
    "            lambda a, m, b: f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$ & \\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {m:.2f}$'\n",
    "                            if b else f'\\\\footnotesize ${a:.2f}$ & \\\\footnotesize ${m:.2f}$'\n",
    "        )(avg, metrics.max(axis=-1), best)\n",
    "\n",
    "    else:\n",
    "        err     = np.abs(metrics - avg.reshape(avg.shape + (1,))).mean(axis=-1)\n",
    "        return np.vectorize(\n",
    "            lambda a, e, b: f'\\\\cellcolor\\u007Bblue!15\\u007D\\\\footnotesize $\\\\bf {a:.2f}$ \\\\tiny $\\\\bf\\\\pm {e:.2f}$'\n",
    "                            if b else f'\\\\footnotesize ${a:.2f}$ \\\\tiny $\\\\pm {e:.2f}$'\n",
    "        )(avg, err, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltx_en = metric2latex(metrics_en)\n",
    "ltx_de = metric2latex(metrics_de)\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    row =  f'{model.upper()} &\\n'\n",
    "\n",
    "    if model in metrics_en:     row += ' & '.join(ltx_en[i])\n",
    "    else:                       row += ' &'*(len(METRICS)-1)\n",
    "    row += ' &\\n'\n",
    "\n",
    "    if model in metrics_de:     row += ' & '.join(ltx_de[i])\n",
    "    else:                       row += ' &'*(len(METRICS)-1)\n",
    "    row += ' \\\\\\\\\\n'\n",
    "\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('semeval': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b666b6ffdea20fd94a09543e671feb1173b4f04c4549159b4e01bb8315246c89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
